<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/images/navbar_logo.svg"/><title>LLM ask AI v0.1.0 - Espanso Hub</title><meta name="description" content="An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field."/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><link rel="modulepreload" href="/assets/entry.client-B_aoCoVg.js"/><link rel="modulepreload" href="/assets/chunk-OIYGIGL5-Ba8ztz7e.js"/><link rel="modulepreload" href="/assets/index-DyJuW5r9.js"/><link rel="modulepreload" href="/assets/root-DtRlfmA7.js"/><link rel="modulepreload" href="/assets/createLucideIcon-Dp8V93Bi.js"/><link rel="modulepreload" href="/assets/package-B6r7Ytj8.js"/><link rel="modulepreload" href="/assets/card-BGu7yhDX.js"/><link rel="modulepreload" href="/assets/separator-COXCAjto.js"/><link rel="modulepreload" href="/assets/shield-Ba_v_8wr.js"/><link rel="stylesheet" href="/assets/root-BHCQwv_u.css"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&amp;display=swap"/></head><body><div class="min-h-screen flex flex-col"><header class="w-full border-b border-primary/30 border-none text-primary-foreground bg-primary sticky top-0 z-50"><div class="content-row"><div class="flex h-16 items-center gap-4"><a class="flex items-center gap-2" href="/" data-discover="true"><img src="/images/navbar_logo.svg" alt="Espanso Hub" class="h-6 md:h-7 w-auto"/><span class="sr-only">Espanso Hub</span></a><div class="flex-1 max-w-md relative"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search absolute left-3 top-1/2 -translate-y-1/2 h-4 w-4 text-muted-foreground pointer-events-none" aria-hidden="true"><path d="m21 21-4.34-4.34"></path><circle cx="11" cy="11" r="8"></circle></svg><input type="text" data-slot="input" class="file:text-foreground selection:bg-primary selection:text-primary-foreground dark:bg-input/30 border-input h-9 min-w-0 rounded-md border px-3 py-1 text-base shadow-xs transition-[color,box-shadow] outline-none file:inline-flex file:h-7 file:border-0 file:bg-transparent file:text-sm file:font-medium disabled:pointer-events-none disabled:cursor-not-allowed disabled:opacity-50 md:text-sm focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive pl-10 w-full bg-background text-foreground placeholder:text-muted-foreground" placeholder="Search for packages" value=""/></div><div class="hidden md:flex flex-1"></div><nav class="hidden md:flex items-center gap-6"><a href="https://espanso.org/docs/get-started/" class="text-sm font-medium text-primary-foreground/90 hover:text-primary-foreground transition-colors" target="_blank" rel="noopener noreferrer">Documentation</a><a href="https://espanso.org/docs/next/packages/creating-a-package/" class="text-sm font-medium text-primary-foreground/90 hover:text-primary-foreground transition-colors" target="_blank" rel="noopener noreferrer">Create Package</a><a class="text-sm font-medium text-primary-foreground/90 hover:text-primary-foreground transition-colors" href="/search" data-discover="true">Explore</a></nav><div class="md:hidden"></div></div></div></header><div class="bg-white py-8"><div class="content-row"><div class="space-y-4"><div class="flex flex-col md:flex-row md:justify-between md:items-center gap-4"><div class="flex items-center gap-2 md:gap-8"><h1 class="text-3xl md:text-5xl font-bold">LLM ask AI</h1></div><div class="flex items-center gap-2 self-start md:self-auto"><button data-slot="button" class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 size-9"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share2 lucide-share-2 h-4 w-4" aria-hidden="true"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" x2="15.42" y1="13.51" y2="17.49"></line><line x1="15.41" x2="8.59" y1="6.51" y2="10.49"></line></svg></button><a href="https://github.com/bgeneto/espanso-llm-ask-ai" target="_blank" rel="noopener noreferrer" aria-label="View on GitHub" data-slot="button" class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 size-9"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-4 w-4" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><button type="button" role="combobox" aria-controls="radix-_R_7535_" aria-expanded="false" aria-autocomplete="none" dir="ltr" data-state="closed" data-slot="select-trigger" data-size="default" class="border-input data-[placeholder]:text-muted-foreground [&amp;_svg:not([class*=&#x27;text-&#x27;])]:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 dark:hover:bg-input/50 flex items-center justify-between gap-2 rounded-md border bg-transparent px-3 py-2 text-sm whitespace-nowrap shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 data-[size=default]:h-9 data-[size=sm]:h-8 *:data-[slot=select-value]:line-clamp-1 *:data-[slot=select-value]:flex *:data-[slot=select-value]:items-center *:data-[slot=select-value]:gap-2 [&amp;_svg]:pointer-events-none [&amp;_svg]:shrink-0 [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 w-[130px] md:w-auto">Select version<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down size-4 opacity-50" aria-hidden="true"><path d="m6 9 6 6 6-6"></path></svg></button><select aria-hidden="true" tabindex="-1" style="position:absolute;border:0;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;word-wrap:normal"></select></div></div><div class="flex flex-col md:flex-row md:items-end gap-6 mt-4"><div class="flex flex-1 flex-col gap-4 md:gap-5"><p class="font-mono text-sm text-muted-foreground">llm-ask-ai</p><p class="text-sm text-muted-foreground">by <!-- -->Bernhard Enders</p><p class="text-base leading-relaxed">An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.</p><div class="flex flex-wrap gap-2"><span data-slot="badge" class="inline-flex items-center justify-center rounded-full border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 cursor-pointer hover:bg-secondary/80">LLM</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-full border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 cursor-pointer hover:bg-secondary/80">AI</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-full border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 cursor-pointer hover:bg-secondary/80">artificial intelligence</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-full border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 cursor-pointer hover:bg-secondary/80">prompt</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-full border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 cursor-pointer hover:bg-secondary/80">Espanso</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-full border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 cursor-pointer hover:bg-secondary/80">Ollama</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-full border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 cursor-pointer hover:bg-secondary/80">OpenAI</span></div></div><div class="flex md:w-1/3 flex-col gap-3 md:gap-5 mt-2 md:mt-0"><div class="bg-muted p-3 rounded-md font-mono text-sm break-all">espanso install llm-ask-ai</div><button data-slot="button" class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 h-8 rounded-md gap-1.5 px-3 has-[&gt;svg]:px-2.5 w-full"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy h-4 w-4 mr-2" aria-hidden="true"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg>Copy</button></div></div></div></div></div><div data-orientation="horizontal" role="none" data-slot="separator" class="bg-border shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px"></div><div dir="ltr" data-orientation="horizontal" data-slot="tabs" class="gap-2 content-row py-8 flex-1 flex flex-col"><div data-slot="card" class="bg-card text-card-foreground gap-6 rounded-xl border py-6 shadow-sm flex-1 flex flex-col"><div data-slot="card-header" class="@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6"><div data-slot="card-title" class="leading-none font-semibold"><div role="tablist" aria-orientation="horizontal" data-slot="tabs-list" class="bg-muted text-muted-foreground inline-flex h-9 w-fit items-center justify-center rounded-lg p-[3px]" tabindex="-1" data-orientation="horizontal" style="outline:none"><button type="button" role="tab" aria-selected="true" aria-controls="radix-_R_235_-content-description" data-state="active" id="radix-_R_235_-trigger-description" data-slot="tabs-trigger" class="data-[state=active]:bg-background dark:data-[state=active]:text-foreground focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:outline-ring dark:data-[state=active]:border-input dark:data-[state=active]:bg-input/30 text-foreground dark:text-muted-foreground inline-flex h-[calc(100%-1px)] flex-1 items-center justify-center gap-1.5 rounded-md border border-transparent px-2 py-1 text-sm font-medium whitespace-nowrap transition-[color,box-shadow] focus-visible:ring-[3px] focus-visible:outline-1 disabled:pointer-events-none disabled:opacity-50 data-[state=active]:shadow-sm [&amp;_svg]:pointer-events-none [&amp;_svg]:shrink-0 [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4" tabindex="-1" data-orientation="horizontal" data-radix-collection-item="">Description</button><button type="button" role="tab" aria-selected="false" aria-controls="radix-_R_235_-content-source" data-state="inactive" id="radix-_R_235_-trigger-source" data-slot="tabs-trigger" class="data-[state=active]:bg-background dark:data-[state=active]:text-foreground focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:outline-ring dark:data-[state=active]:border-input dark:data-[state=active]:bg-input/30 text-foreground dark:text-muted-foreground inline-flex h-[calc(100%-1px)] flex-1 items-center justify-center gap-1.5 rounded-md border border-transparent px-2 py-1 text-sm font-medium whitespace-nowrap transition-[color,box-shadow] focus-visible:ring-[3px] focus-visible:outline-1 disabled:pointer-events-none disabled:opacity-50 data-[state=active]:shadow-sm [&amp;_svg]:pointer-events-none [&amp;_svg]:shrink-0 [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4" tabindex="-1" data-orientation="horizontal" data-radix-collection-item="">Source</button></div></div></div><div data-slot="card-content" class="px-6 flex-1"><div data-state="active" data-orientation="horizontal" role="tabpanel" aria-labelledby="radix-_R_235_-trigger-description" id="radix-_R_235_-content-description" tabindex="0" data-slot="tabs-content" class="flex-1 outline-none h-full flex flex-col" style="animation-duration:0s"><p class="text-base leading-relaxed whitespace-pre-wrap"><h1 class="text-4xl font-bold" node="[object Object]">llm-ask-ai</h1>
<p>An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.</p>
<h2 class="text-3xl font-bold" node="[object Object]">Requirements</h2>
<ul>
<li><a href="https://espanso.org/">Espanso</a> installed and running</li>
<li>Python 3.9+ installed and available on your system path</li>
<li>Required Python packages: <code>openai</code> and <code>python-dotenv</code> (see <code>requirements.txt</code>)</li>
<li>Access to a local LLM (such as <a href="https://ollama.com/">Ollama</a> or <a href="https://lmstudio.ai/">LM Studio</a>) or a remote OpenAI compatible API (in this case, you will need to provide your API key in the <code>.env</code> file)</li>
</ul>
<h2 class="text-3xl font-bold" node="[object Object]">Configuration</h2>
<p>Edit the <code>.env</code> environment file inside the package directory () to set the <code>API_KEY</code>, <code>BASE_URL</code>and <code>MODEL</code>. Example:</p>
<pre><code class="language-bash">API_KEY=ollama
BASE_URL=http://localhost:11434/v1
MODEL=llama3.2
</code></pre>
<blockquote>
<p>NOTE: don&#x27;t forget to pull the Ollama model first. In the case above, just issue the command:</p>
<p><code>ollama pull llama3.2</code></p>
</blockquote>
<h2 class="text-3xl font-bold" node="[object Object]">Usage</h2>
<ul>
<li>Type the Espanso trigger for this package (<code>:ask:ai</code>) in any text field.</li>
<li>Enter your desired AI prompt when asked.</li>
<li>The AI-generated response will be inserted automatically (this can take several seconds, so choose a small and low latency LLM model).</li>
</ul>
<h3 class="text-2xl font-bold" node="[object Object]">Example</h3>
<p>Type:</p>
<pre><code>:ask:ai
</code></pre>
<p>Then enter:</p>
<pre><code>Summarize the following text: ...
</code></pre>
<p>Press the &quot;Submit&quot; button or &quot;CTRL + Enter&quot; to send the request. The response from your configured LLM will appear in place.</p>
<h2 class="text-3xl font-bold" node="[object Object]">Troubleshooting</h2>
<ul>
<li>
<p>Make sure <code>python</code> (not <code>python3</code>) executable is available globally on your system&#x27;s PATH environment variable and that the required packages are installed.</p>
</li>
<li>
<p>Certain Linux distributions, such as Debian, use only a <code>python3</code> executable and don&#x27;t include a <code>python</code> executable or symlink. In this case, you may need to install the <code>python-is-python3</code> package (recommended), create a symbolic link, or run a command like this:</p>
<pre><code class="language-bash">sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10
</code></pre>
</li>
<li>
<p>Make sure you have the necessary Python packages installed globally, either by running <code>pip install openai python-dotenv</code> or by using a command like <code>sudo apt install python3-openai python3-dotenv</code>.</p>
</li>
<li>
<p>Check that your BASE_URL endpoint, MODEL and API_KEY are correctly set in the <code>.env</code> file (located in the Espanso config directory: <code>%CONFIG%/match/packages/llm-ask-ai/.env</code>).</p>
</li>
<li>
<p>After sending the request, make sure the cursor doesn’t lose focus and remains blinking at the correct insertion point. If it doesn’t, just click to place it right after the trigger string like this <strong>:ask:ai|</strong></p>
</li>
<li>
<p>Review Espanso logs for errors.</p>
</li>
</ul>
<h2 class="text-3xl font-bold" node="[object Object]">License</h2>
<p>MIT</p>
<h2 class="text-3xl font-bold" node="[object Object]">Author</h2>
<p>Bernhard Enders</p>
<h2 class="text-3xl font-bold" node="[object Object]">Links</h2>
<ul>
<li><a href="https://github.com/bgeneto/espanso-llm-ask-ai">Homepage</a></li>
<li><a href="https://espanso.org/docs/">Espanso Documentation</a></li>
</ul></p></div><div data-state="inactive" data-orientation="horizontal" role="tabpanel" aria-labelledby="radix-_R_235_-trigger-source" hidden="" id="radix-_R_235_-content-source" tabindex="0" data-slot="tabs-content" class="flex-1 outline-none h-full flex flex-col"></div></div></div></div></div><footer class="border-t border-primary/20 bg-primary text-primary-foreground"><div class="content-row py-6"><div class="flex flex-col items-center gap-6 md:flex-row md:items-center md:justify-between"><a class="flex items-center gap-2" href="/" data-discover="true"><img src="/images/navbar_logo.svg" alt="Espanso Hub" class="h-7 w-auto"/><span class="sr-only">Espanso Hub</span></a><a href="https://espanso.org/docs/get-started/" target="_blank" rel="noopener noreferrer" class="text-sm font-medium text-primary-foreground/90 hover:text-primary-foreground transition-colors">Documentation</a><a href="https://espanso.org/docs/next/packages/creating-a-package/" target="_blank" rel="noopener noreferrer" class="text-sm font-medium text-primary-foreground/90 hover:text-primary-foreground transition-colors">Create Package</a><a class="text-sm font-medium text-primary-foreground/90 hover:text-primary-foreground transition-colors" href="/search" data-discover="true">Explore</a><a href="https://github.com/espanso/hub-frontend/" target="_blank" rel="noopener noreferrer" class="text-sm font-medium text-primary-foreground/90 hover:text-primary-foreground transition-colors">Contribute</a><a href="https://espanso.org" target="_blank" rel="noopener noreferrer" class="text-sm font-medium text-primary-foreground/90 hover:text-primary-foreground transition-colors">Espanso</a><a href="https://www.reddit.com/r/espanso/" target="_blank" rel="noopener noreferrer" class="text-sm font-medium text-primary-foreground/90 hover:text-primary-foreground transition-colors">Reddit</a></div><div class="mt-6 flex items-center justify-center gap-2 text-sm text-primary-foreground/90"><span>Made with</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart h-4 w-4" aria-hidden="true"><path d="M2 9.5a5.5 5.5 0 0 1 9.591-3.676.56.56 0 0 0 .818 0A5.49 5.49 0 0 1 22 9.5c0 2.29-1.5 4-3 5.5l-5.492 5.313a2 2 0 0 1-3 .019L5 15c-1.5-1.5-3-3.2-3-5.5"></path></svg><span>by</span><a href="https://www.matteopellegrino.dev/" target="_blank" rel="noopener noreferrer" class="font-medium underline-offset-4 hover:underline">Matteo Pellegrino</a></div></div></footer><script>((storageKey2, restoreKey) => {
    if (!window.history.state || !window.history.state.key) {
      let key = Math.random().toString(32).slice(2);
      window.history.replaceState({ key }, "");
    }
    try {
      let positions = JSON.parse(sessionStorage.getItem(storageKey2) || "{}");
      let storedY = positions[restoreKey || window.history.state.key];
      if (typeof storedY === "number") {
        window.scrollTo(0, storedY);
      }
    } catch (error) {
      console.error(error);
      sessionStorage.removeItem(storageKey2);
    }
  })("react-router-scroll-positions", null)</script><script>window.__reactRouterContext = {"basename":"/","future":{"v8_middleware":false,"unstable_optimizeDeps":false,"unstable_splitRouteModules":false,"unstable_subResourceIntegrity":false,"unstable_viteEnvironmentApi":false},"routeDiscovery":{"mode":"lazy","manifestPath":"/__manifest"},"ssr":true,"isSpaMode":false};window.__reactRouterContext.stream = new ReadableStream({start(controller){window.__reactRouterContext.streamController = controller;}}).pipeThrough(new TextEncoderStream());</script><script type="module" async="">;
import * as route0 from "/assets/root-DtRlfmA7.js";
import * as route1 from "/assets/package-B6r7Ytj8.js";
  window.__reactRouterManifest = {
  "entry": {
    "module": "/assets/entry.client-B_aoCoVg.js",
    "imports": [
      "/assets/chunk-OIYGIGL5-Ba8ztz7e.js",
      "/assets/index-DyJuW5r9.js"
    ],
    "css": []
  },
  "routes": {
    "root": {
      "id": "root",
      "path": "",
      "hasAction": false,
      "hasLoader": false,
      "hasClientAction": false,
      "hasClientLoader": false,
      "hasClientMiddleware": false,
      "hasErrorBoundary": true,
      "module": "/assets/root-DtRlfmA7.js",
      "imports": [
        "/assets/chunk-OIYGIGL5-Ba8ztz7e.js",
        "/assets/index-DyJuW5r9.js",
        "/assets/createLucideIcon-Dp8V93Bi.js"
      ],
      "css": [
        "/assets/root-BHCQwv_u.css"
      ]
    },
    "package-version": {
      "id": "package-version",
      "parentId": "root",
      "path": ":packageName/v/:version",
      "hasAction": false,
      "hasLoader": true,
      "hasClientAction": false,
      "hasClientLoader": false,
      "hasClientMiddleware": false,
      "hasErrorBoundary": false,
      "module": "/assets/package-B6r7Ytj8.js",
      "imports": [
        "/assets/chunk-OIYGIGL5-Ba8ztz7e.js",
        "/assets/card-BGu7yhDX.js",
        "/assets/separator-COXCAjto.js",
        "/assets/index-DyJuW5r9.js",
        "/assets/shield-Ba_v_8wr.js",
        "/assets/createLucideIcon-Dp8V93Bi.js"
      ],
      "css": []
    },
    "routes/home": {
      "id": "routes/home",
      "parentId": "root",
      "index": true,
      "hasAction": false,
      "hasLoader": true,
      "hasClientAction": false,
      "hasClientLoader": false,
      "hasClientMiddleware": false,
      "hasErrorBoundary": false,
      "module": "/assets/home-4hEkyLJ0.js",
      "imports": [
        "/assets/chunk-OIYGIGL5-Ba8ztz7e.js",
        "/assets/card-BGu7yhDX.js",
        "/assets/PackageCard-B8NdNGsH.js",
        "/assets/shield-Ba_v_8wr.js",
        "/assets/createLucideIcon-Dp8V93Bi.js"
      ],
      "css": []
    },
    "routes/package": {
      "id": "routes/package",
      "parentId": "root",
      "path": ":packageName",
      "hasAction": false,
      "hasLoader": true,
      "hasClientAction": false,
      "hasClientLoader": false,
      "hasClientMiddleware": false,
      "hasErrorBoundary": false,
      "module": "/assets/package-B6r7Ytj8.js",
      "imports": [
        "/assets/chunk-OIYGIGL5-Ba8ztz7e.js",
        "/assets/card-BGu7yhDX.js",
        "/assets/separator-COXCAjto.js",
        "/assets/index-DyJuW5r9.js",
        "/assets/shield-Ba_v_8wr.js",
        "/assets/createLucideIcon-Dp8V93Bi.js"
      ],
      "css": []
    }
  },
  "url": "/assets/manifest-df0f0420.js",
  "version": "df0f0420"
};
  window.__reactRouterRouteModules = {"root":route0,"package-version":route1};

import("/assets/entry.client-B_aoCoVg.js");</script><!--$?--><template id="B:0"></template><!--/$--><script id="_R_">requestAnimationFrame(function(){$RT=performance.now()});</script><div hidden id="S:0"><script>window.__reactRouterContext.streamController.enqueue("[{\"_1\":2,\"_52\":-5,\"_53\":-5},\"loaderData\",{\"_3\":4},\"package-version\",{\"_5\":6,\"_48\":49,\"_50\":51},\"package\",{\"_7\":8,\"_9\":10,\"_11\":12,\"_13\":14,\"_15\":16,\"_17\":18,\"_19\":20,\"_21\":22,\"_30\":31,\"_32\":33,\"_46\":47},\"name\",\"llm-ask-ai\",\"author\",\"Bernhard Enders\",\"description\",\"An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.\",\"title\",\"LLM ask AI\",\"version\",\"0.1.0\",\"archive_url\",\"https://github.com/espanso/hub/releases/latest/download/llm-ask-ai-0.1.0.zip\",\"archive_sha256_url\",\"https://github.com/espanso/hub/releases/latest/download/llm-ask-ai-0.1.0-sha256.txt\",\"tags\",[23,24,25,26,27,28,29],\"LLM\",\"AI\",\"artificial intelligence\",\"prompt\",\"Espanso\",\"Ollama\",\"OpenAI\",\"id\",\"llm-ask-ai-0.1.0\",\"files\",{\"_34\":35,\"_36\":37,\"_38\":39,\"_40\":41,\"_42\":43,\"_44\":45},\"requirements.txt\",\"openai\\npython-dotenv\\n\",\"example.env\",\"API_KEY=ollama\\nBASE_URL=http://localhost:11434/v1\\nMODEL=llama3.2\",\"README.md\",\"# llm-ask-ai\\n\\nAn Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.\\n\\n## Requirements\\n\\n- [Espanso](https://espanso.org/) installed and running\\n- Python 3.9+ installed and available on your system path\\n- Required Python packages: `openai` and `python-dotenv` (see `requirements.txt`)\\n- Access to a local LLM (such as [Ollama](https://ollama.com/) or [LM Studio](https://lmstudio.ai/)) or a remote OpenAI compatible API (in this case, you will need to provide your API key in the `.env` file)\\n\\n## Configuration\\n\\nEdit the `.env` environment file inside the package directory () to set the `API_KEY`, `BASE_URL`and `MODEL`. Example:\\n\\n```bash\\nAPI_KEY=ollama\\nBASE_URL=http://localhost:11434/v1\\nMODEL=llama3.2\\n```\\n\\n\u003e NOTE: don't forget to pull the Ollama model first. In the case above, just issue the command:\\n\u003e\\n\u003e `ollama pull llama3.2`\\n\\n## Usage\\n\\n- Type the Espanso trigger for this package (`:ask:ai`) in any text field.\\n- Enter your desired AI prompt when asked.\\n- The AI-generated response will be inserted automatically (this can take several seconds, so choose a small and low latency LLM model).\\n\\n### Example\\n\\nType:\\n```\\n:ask:ai\\n```\\nThen enter:\\n```\\nSummarize the following text: ...\\n```\\nPress the \\\"Submit\\\" button or \\\"CTRL + Enter\\\" to send the request. The response from your configured LLM will appear in place.\\n\\n## Troubleshooting\\n\\n- Make sure `python` (not `python3`) executable is available globally on your system's PATH environment variable and that the required packages are installed.\\n- Certain Linux distributions, such as Debian, use only a `python3` executable and don't include a `python` executable or symlink. In this case, you may need to install the `python-is-python3` package (recommended), create a symbolic link, or run a command like this:\\n  ```bash\\n  sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10\\n  ```\\n\\n- Make sure you have the necessary Python packages installed globally, either by running `pip install openai python-dotenv` or by using a command like `sudo apt install python3-openai python3-dotenv`.\\n- Check that your BASE_URL endpoint, MODEL and API_KEY are correctly set in the `.env` file (located in the Espanso config directory: `%CONFIG%/match/packages/llm-ask-ai/.env`).\\n- After sending the request, make sure the cursor doesn’t lose focus and remains blinking at the correct insertion point. If it doesn’t, just click to place it right after the trigger string like this **:ask:ai|**\\n- Review Espanso logs for errors.\\n\\n## License\\n\\nMIT\\n\\n## Author\\n\\nBernhard Enders\\n\\n## Links\\n\\n- [Homepage](https://github.com/bgeneto/espanso-llm-ask-ai)\\n- [Espanso Documentation](https://espanso.org/docs/)\\n\",\"_manifest.yml\",\"name: \\\"llm-ask-ai\\\"\\ntitle: \\\"LLM ask AI\\\"\\ndescription: \\\"An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.\\\"\\nversion: \\\"0.1.0\\\"\\nauthor: \\\"Bernhard Enders\\\"\\nhomepage: \\\"https://github.com/bgeneto/espanso-llm-ask-ai\\\"\\ntags: [\\\"LLM\\\", \\\"AI\\\", \\\"artificial intelligence\\\", \\\"prompt\\\", \\\"Espanso\\\", \\\"Ollama\\\", \\\"OpenAI\\\"]\",\"package.yml\",\"matches:\\n  - trigger: \\\":ask:ai\\\"\\n    replace: \\\"{{output}}\\\"\\n    vars:\\n      - name: \\\"prompt\\\"\\n        type: \\\"form\\\"\\n        params:\\n          layout: \\\"What would you like to ask the Artificial intelligence LLM today?\\\\n\\\\n[[text]]\\\"\\n          fields:\\n            text:\\n              multiline: true\\n      - name: \\\"output\\\"\\n        type: \\\"script\\\"\\n        params:\\n          args:\\n            - python\\n            - \\\"%CONFIG%/match/packages/llm-ask-ai/ask-ai.py\\\"\\n            - \\\"{{prompt.text}}\\\"\\n\",\"ask-ai.py\",\"#!/usr/bin/env python\\n\\n\\\"\\\"\\\"\\nask-ai.py: Query a local or remote LLM (Large Language Model) using the OpenAI API interface and return the response.\\n\\nAuthor: Bernhard Enders\\nDate: 2025-06-14\\nVersion: 0.1.0\\n\\nDescription:\\n    This script is designed for integration with Espanso, allowing users to send a prompt to an LLM and receive a direct answer, suitable for text expansion workflows.\\n\\nFeatures:\\n    - Loads configuration (API key, base URL, model) from a .env file in the script directory.\\n    - Sends the user-provided prompt to the LLM with a system message enforcing non-interactive, assumption-based responses.\\n    - Prints the LLM's response to stdout for Espanso to capture.\\n    - Handles errors gracefully and provides clear error messages if configuration or API calls fail.\\n\\nUsage:\\n    python ask-ai.py \\\"\u003ctext\u003e\\\"\\n\\nRequirements:\\n    - openai\\n    - python-dotenv\\n    - Python 3.9+\\n\\\"\\\"\\\"\\n\\nimport os\\nimport shutil\\nimport sys\\nsys.stdout.reconfigure(encoding=\\\"utf-8\\\")\\n\\n# packages dependency check\\nREQUIRED_PACKAGES = [\\\"openai\\\", \\\"dotenv\\\"]\\nmissing = []\\nfor pkg in REQUIRED_PACKAGES:\\n    try:\\n        __import__(pkg)\\n    except ImportError:\\n        missing.append(pkg)\\nif missing:\\n    print(f\\\"❌ Error: Missing required packages: {', '.join(missing)}. Please install them with 'pip install -r requirements.txt'.\\\")\\n    sys.exit(0)\\n\\nfrom openai import OpenAI\\nfrom dotenv import load_dotenv\\n\\n\\ndef ensure_env_file() -\u003e None:\\n    \\\"\\\"\\\"\\n    Ensures that a .env file exists. If not, tries to copy example.env to .env using shutil.copy.\\n    Raises FileNotFoundError if neither exists, lets OSError propagate on copy failure.\\n    \\\"\\\"\\\"\\n    script_dir = os.path.dirname(__file__)\\n    env_path = os.path.join(script_dir, \\\".env\\\")\\n    example_env_path = os.path.join(script_dir, \\\"example.env\\\")\\n\\n    if not os.path.exists(env_path):\\n        if os.path.exists(example_env_path):\\n            shutil.copy(example_env_path, env_path)\\n        else:\\n            raise FileNotFoundError(f\\\"❌ Error: Missing .env file in directory: '{script_dir}'.\\\")\\n\\ndef load_and_validate_env() -\u003e tuple[str, str, str]:\\n    \\\"\\\"\\\"\\n    Loads and validates .env configuration. Returns (api_key, base_url, model).\\n    Raises ValueError if any required variable is missing.\\n    \\\"\\\"\\\"\\n    dotenv_path = os.path.join(os.path.dirname(__file__), \\\".env\\\")\\n    load_dotenv(dotenv_path)\\n    api_key = os.getenv(\\\"API_KEY\\\")\\n    base_url = os.getenv(\\\"BASE_URL\\\")\\n    model = os.getenv(\\\"MODEL\\\")\\n    if not api_key:\\n        raise ValueError(\\\"❌ Error: API_KEY variable not found in .env file!\\\")\\n    if not base_url:\\n        raise ValueError(\\\"❌ Error: BASE_URL variable not found in .env file!\\\")\\n    if not model:\\n        raise ValueError(\\\"❌ Error: MODEL variable not found in .env file!\\\")\\n    return api_key, base_url, model\\n\\ndef ask_ai(text: str) -\u003e str:\\n    \\\"\\\"\\\"\\n    Sends a prompt to a local or remote LLM using the OpenAI API interface and returns the response.\\n\\n    Args:\\n        text (str): The user prompt to send to the LLM.\\n\\n    Returns:\\n        str: The LLM's response or an error message.\\n    \\\"\\\"\\\"\\n    try:\\n        ensure_env_file()\\n        api_key, base_url, model = load_and_validate_env()\\n        client = OpenAI(api_key=api_key, base_url=base_url)\\n        response = client.chat.completions.create(\\n            model=model,\\n            messages=[\\n                {\\n                    \\\"role\\\": \\\"system\\\",\\n                    \\\"content\\\": (\\n                        \\\"You are operating in a non-interactive mode.\\\\n\\\"\\n                        \\\"Do NOT use introductory phrases, greetings, or opening messages.\\\\n\\\"\\n                        \\\"You CANNOT ask the user for clarification, additional details, or preferences.\\\\n\\\"\\n                        \\\"When given a request, make reasonable assumptions based on the context and provide a complete, helpful response immediately.\\\\n\\\"\\n                        \\\"If a request is ambiguous, choose the most common or logical interpretation and proceed accordingly.\\\\n\\\"\\n                        \\\"Always deliver a substantive response rather than asking questions.\\\\n\\\"\\n                        \\\"NEVER ask the user for follow-up questions or clarifications.\\\"\\n                    ),\\n                },\\n                {\\n                    \\\"role\\\": \\\"user\\\",\\n                    \\\"content\\\": text,\\n                },\\n            ],\\n            stream=False,\\n        )\\n        answer = response.choices[0].message.content.strip()\\n        return answer\\n    except Exception:\\n        return \\\"❌ Error: An unexpected error occurred while processing your request. Check model name, api key etc... and try again later.\\\"\\n\\ndef main() -\u003e None:\\n    \\\"\\\"\\\"\\n    Main entry point for the script. Parses arguments, validates input, calls ask_ai, and prints the result.\\n    \\\"\\\"\\\"\\n    if len(sys.argv) != 2:\\n        print('❌ Usage error: python ask-ai.py \\\"\u003ctext\u003e\\\"')\\n        return\\n    text = sys.argv[1]\\n    if not text.strip():\\n        print(\\\"❌ Error: No prompt text provided!\\\")\\n        return\\n    result = ask_ai(text)\\n    print(result)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\",\"repositoryHome\",\"https://github.com/bgeneto/espanso-llm-ask-ai\",\"versions\",[16],\"isLatest\",true,\"actionData\",\"errors\"]\n");</script><!--$?--><template id="B:1"></template><!--/$--></div><script>$RB=[];$RV=function(a){$RT=performance.now();for(var b=0;b<a.length;b+=2){var c=a[b],e=a[b+1];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g._reactRetry&&requestAnimationFrame(g._reactRetry)}}a.length=0};
$RC=function(a,b){if(b=document.getElementById(b))(a=document.getElementById(a))?(a.previousSibling.data="$~",$RB.push(a,b),2===$RB.length&&("number"!==typeof $RT?requestAnimationFrame($RV.bind(null,$RB)):(a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:$RT+300-a)))):b.parentNode.removeChild(b)};$RC("B:0","S:0")</script><div hidden id="S:1"><script>window.__reactRouterContext.streamController.close();</script></div><script>$RC("B:1","S:1")</script></body></html>