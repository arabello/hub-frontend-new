[{"_1":2},"routes/package",{"_3":4},"data",{"_5":6,"_48":49,"_50":51},"package",{"_7":8,"_9":10,"_11":12,"_13":14,"_15":16,"_17":18,"_19":20,"_21":22,"_30":31,"_32":33,"_46":47},"name","llm-ask-ai","author","Bernhard Enders","description","An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.","title","LLM ask AI","version","0.1.0","archive_url","https://github.com/espanso/hub/releases/latest/download/llm-ask-ai-0.1.0.zip","archive_sha256_url","https://github.com/espanso/hub/releases/latest/download/llm-ask-ai-0.1.0-sha256.txt","tags",[23,24,25,26,27,28,29],"LLM","AI","artificial intelligence","prompt","Espanso","Ollama","OpenAI","id","llm-ask-ai-0.1.0","files",{"_34":35,"_36":37,"_38":39,"_40":41,"_42":43,"_44":45},"requirements.txt","openai\npython-dotenv\n","example.env","API_KEY=ollama\nBASE_URL=http://localhost:11434/v1\nMODEL=llama3.2","README.md","# llm-ask-ai\n\nAn Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.\n\n## Requirements\n\n- [Espanso](https://espanso.org/) installed and running\n- Python 3.9+ installed and available on your system path\n- Required Python packages: `openai` and `python-dotenv` (see `requirements.txt`)\n- Access to a local LLM (such as [Ollama](https://ollama.com/) or [LM Studio](https://lmstudio.ai/)) or a remote OpenAI compatible API (in this case, you will need to provide your API key in the `.env` file)\n\n## Configuration\n\nEdit the `.env` environment file inside the package directory () to set the `API_KEY`, `BASE_URL`and `MODEL`. Example:\n\n```bash\nAPI_KEY=ollama\nBASE_URL=http://localhost:11434/v1\nMODEL=llama3.2\n```\n\n> NOTE: don't forget to pull the Ollama model first. In the case above, just issue the command:\n>\n> `ollama pull llama3.2`\n\n## Usage\n\n- Type the Espanso trigger for this package (`:ask:ai`) in any text field.\n- Enter your desired AI prompt when asked.\n- The AI-generated response will be inserted automatically (this can take several seconds, so choose a small and low latency LLM model).\n\n### Example\n\nType:\n```\n:ask:ai\n```\nThen enter:\n```\nSummarize the following text: ...\n```\nPress the \"Submit\" button or \"CTRL + Enter\" to send the request. The response from your configured LLM will appear in place.\n\n## Troubleshooting\n\n- Make sure `python` (not `python3`) executable is available globally on your system's PATH environment variable and that the required packages are installed.\n- Certain Linux distributions, such as Debian, use only a `python3` executable and don't include a `python` executable or symlink. In this case, you may need to install the `python-is-python3` package (recommended), create a symbolic link, or run a command like this:\n  ```bash\n  sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10\n  ```\n\n- Make sure you have the necessary Python packages installed globally, either by running `pip install openai python-dotenv` or by using a command like `sudo apt install python3-openai python3-dotenv`.\n- Check that your BASE_URL endpoint, MODEL and API_KEY are correctly set in the `.env` file (located in the Espanso config directory: `%CONFIG%/match/packages/llm-ask-ai/.env`).\n- After sending the request, make sure the cursor doesn’t lose focus and remains blinking at the correct insertion point. If it doesn’t, just click to place it right after the trigger string like this **:ask:ai|**\n- Review Espanso logs for errors.\n\n## License\n\nMIT\n\n## Author\n\nBernhard Enders\n\n## Links\n\n- [Homepage](https://github.com/bgeneto/espanso-llm-ask-ai)\n- [Espanso Documentation](https://espanso.org/docs/)\n","_manifest.yml","name: \"llm-ask-ai\"\ntitle: \"LLM ask AI\"\ndescription: \"An Espanso package that enables users to quickly send prompts to a local (e.g. Ollama or LM Studio) or remote LLM API calls (OpenAI standard) and insert the AI-generated response directly into any text field.\"\nversion: \"0.1.0\"\nauthor: \"Bernhard Enders\"\nhomepage: \"https://github.com/bgeneto/espanso-llm-ask-ai\"\ntags: [\"LLM\", \"AI\", \"artificial intelligence\", \"prompt\", \"Espanso\", \"Ollama\", \"OpenAI\"]","package.yml","matches:\n  - trigger: \":ask:ai\"\n    replace: \"{{output}}\"\n    vars:\n      - name: \"prompt\"\n        type: \"form\"\n        params:\n          layout: \"What would you like to ask the Artificial intelligence LLM today?\\n\\n[[text]]\"\n          fields:\n            text:\n              multiline: true\n      - name: \"output\"\n        type: \"script\"\n        params:\n          args:\n            - python\n            - \"%CONFIG%/match/packages/llm-ask-ai/ask-ai.py\"\n            - \"{{prompt.text}}\"\n","ask-ai.py","#!/usr/bin/env python\n\n\"\"\"\nask-ai.py: Query a local or remote LLM (Large Language Model) using the OpenAI API interface and return the response.\n\nAuthor: Bernhard Enders\nDate: 2025-06-14\nVersion: 0.1.0\n\nDescription:\n    This script is designed for integration with Espanso, allowing users to send a prompt to an LLM and receive a direct answer, suitable for text expansion workflows.\n\nFeatures:\n    - Loads configuration (API key, base URL, model) from a .env file in the script directory.\n    - Sends the user-provided prompt to the LLM with a system message enforcing non-interactive, assumption-based responses.\n    - Prints the LLM's response to stdout for Espanso to capture.\n    - Handles errors gracefully and provides clear error messages if configuration or API calls fail.\n\nUsage:\n    python ask-ai.py \"<text>\"\n\nRequirements:\n    - openai\n    - python-dotenv\n    - Python 3.9+\n\"\"\"\n\nimport os\nimport shutil\nimport sys\nsys.stdout.reconfigure(encoding=\"utf-8\")\n\n# packages dependency check\nREQUIRED_PACKAGES = [\"openai\", \"dotenv\"]\nmissing = []\nfor pkg in REQUIRED_PACKAGES:\n    try:\n        __import__(pkg)\n    except ImportError:\n        missing.append(pkg)\nif missing:\n    print(f\"❌ Error: Missing required packages: {', '.join(missing)}. Please install them with 'pip install -r requirements.txt'.\")\n    sys.exit(0)\n\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\n\ndef ensure_env_file() -> None:\n    \"\"\"\n    Ensures that a .env file exists. If not, tries to copy example.env to .env using shutil.copy.\n    Raises FileNotFoundError if neither exists, lets OSError propagate on copy failure.\n    \"\"\"\n    script_dir = os.path.dirname(__file__)\n    env_path = os.path.join(script_dir, \".env\")\n    example_env_path = os.path.join(script_dir, \"example.env\")\n\n    if not os.path.exists(env_path):\n        if os.path.exists(example_env_path):\n            shutil.copy(example_env_path, env_path)\n        else:\n            raise FileNotFoundError(f\"❌ Error: Missing .env file in directory: '{script_dir}'.\")\n\ndef load_and_validate_env() -> tuple[str, str, str]:\n    \"\"\"\n    Loads and validates .env configuration. Returns (api_key, base_url, model).\n    Raises ValueError if any required variable is missing.\n    \"\"\"\n    dotenv_path = os.path.join(os.path.dirname(__file__), \".env\")\n    load_dotenv(dotenv_path)\n    api_key = os.getenv(\"API_KEY\")\n    base_url = os.getenv(\"BASE_URL\")\n    model = os.getenv(\"MODEL\")\n    if not api_key:\n        raise ValueError(\"❌ Error: API_KEY variable not found in .env file!\")\n    if not base_url:\n        raise ValueError(\"❌ Error: BASE_URL variable not found in .env file!\")\n    if not model:\n        raise ValueError(\"❌ Error: MODEL variable not found in .env file!\")\n    return api_key, base_url, model\n\ndef ask_ai(text: str) -> str:\n    \"\"\"\n    Sends a prompt to a local or remote LLM using the OpenAI API interface and returns the response.\n\n    Args:\n        text (str): The user prompt to send to the LLM.\n\n    Returns:\n        str: The LLM's response or an error message.\n    \"\"\"\n    try:\n        ensure_env_file()\n        api_key, base_url, model = load_and_validate_env()\n        client = OpenAI(api_key=api_key, base_url=base_url)\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You are operating in a non-interactive mode.\\n\"\n                        \"Do NOT use introductory phrases, greetings, or opening messages.\\n\"\n                        \"You CANNOT ask the user for clarification, additional details, or preferences.\\n\"\n                        \"When given a request, make reasonable assumptions based on the context and provide a complete, helpful response immediately.\\n\"\n                        \"If a request is ambiguous, choose the most common or logical interpretation and proceed accordingly.\\n\"\n                        \"Always deliver a substantive response rather than asking questions.\\n\"\n                        \"NEVER ask the user for follow-up questions or clarifications.\"\n                    ),\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": text,\n                },\n            ],\n            stream=False,\n        )\n        answer = response.choices[0].message.content.strip()\n        return answer\n    except Exception:\n        return \"❌ Error: An unexpected error occurred while processing your request. Check model name, api key etc... and try again later.\"\n\ndef main() -> None:\n    \"\"\"\n    Main entry point for the script. Parses arguments, validates input, calls ask_ai, and prints the result.\n    \"\"\"\n    if len(sys.argv) != 2:\n        print('❌ Usage error: python ask-ai.py \"<text>\"')\n        return\n    text = sys.argv[1]\n    if not text.strip():\n        print(\"❌ Error: No prompt text provided!\")\n        return\n    result = ask_ai(text)\n    print(result)\n\nif __name__ == \"__main__\":\n    main()\n","repositoryHome","https://github.com/bgeneto/espanso-llm-ask-ai","versions",[16],"isLatest",true]
